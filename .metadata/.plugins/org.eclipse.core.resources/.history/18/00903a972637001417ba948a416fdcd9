package org.allenai.relation.learning

import java.io.{ InputStream, File }
import scala.io.Source
import scala.collection.mutable.Map
import org.allenai.common.Resource
import org.allenai.ari.solvers.inference.matching.{ EntailmentWrapper, EntailmentService }
import org.allenai.ari.solvers.utils.Tokenizer
import org.allenai.relation.util.Polyparser

object FeatureWrapper {

  private def getResourceAsStream(name: String): InputStream =
    getClass.getClassLoader.getResourceAsStream(name)
    
  //  val wordFrequency = loadWordFrequencies("word-frequencies.txt")
  //  val minFreq = wordFrequency.values.min

  def overlap(text: Set[String], hypothesis: Set[String]) =
    text.intersect(hypothesis).size

  import Tokenizer._
  //  def hypothesisCoverage(text: String, hypothesis: String): Double = {
  //    hypothesisCoverage(toKeywords(text).toSet, toKeywords(hypothesis).toSet)
  //  }

  def distance(sentence: String, arg1: String, arg2: String): Double = {
    val tokens = sentence.split("\\s+").toList
    val arg1s = arg1.split("\\s+").toList
    val arg2s = arg2.split("\\s+").toList
    val arg1sIndex = tokens.indexOfSlice(arg1s)
    val arg2sIndex = tokens.indexOfSlice(arg2s)
    // problems: (1) ...+length(arg1 or arg2); (2) arg1 appears multiple times
    Math.abs(arg1sIndex - arg2sIndex).toDouble
  }
  //  Math.abs(sentence.indexOf(arg1) - sentence.indexOf(arg2)).toDouble

  val wordnetEntailmentService: EntailmentService = {
    val wordnetEntailmentUrl = "http://entailment.dev.allenai.org:8191/api/entails"
    val wrapper = new EntailmentWrapper(wordnetEntailmentUrl)
    wrapper.CachedEntails
  }

  def wordnetEntailment(text: String, hypothesis: String) =
    wordnetEntailmentService(text, hypothesis) map { _.confidence } getOrElse 0d
    
  def wordnetEntailment(text: String, allhypothesis: List[String]) = {
      val entaillist = for {
        hypothesis <- allhypothesis
      } yield { 
        val hypothesistoken = Tokenizer.toKeywords(hypothesis).toSet
        wordnetEntailmentService(text, hypothesistoken) map { _.confidence } getOrElse 0d 
      }
      val entailavg = (0.0 /: entaillist) { _ + _ } / entaillist.length
	  entailavg
    }

  val word2vecEntailmentService: EntailmentService = {
    val word2vecEntailmentUrl = "???"
    val wrapper = new EntailmentWrapper(word2vecEntailmentUrl)
    wrapper.CachedEntails
  }

  def getrootstring(root: Polyparser.Mytokennode): String = {
    var rootstring = "null"
    if (root != null) rootstring = root.string //cannot contain any " " or some special characters
    rootstring
  }

  /** Get the map, where key=dependency-path-length, value=list-of-specific-dependencies
    */
  def getLengthDependencies(root: Polyparser.Mytokennode, tree: Polyparser.Mygraph,
    arg1list: List[Int], arg2list: List[Int]) = {
    val lengthDependenciesMap: Map[Integer, List[(Int, Int, Set[Polyparser.Myedge])]] = collection.mutable.Map.empty
    lengthDependenciesMap.put(1, getpathwithspecificlength(root, tree, arg1list, arg2list, 1))
    lengthDependenciesMap.put(2, getpathwithspecificlength(root, tree, arg1list, arg2list, 2))
    lengthDependenciesMap.put(3, getpathwithspecificlength(root, tree, arg1list, arg2list, 3))

    lengthDependenciesMap
  }
  
  /**
   * load lexical-seeds for each disrel from a local file
   */
  def loadLexicalSeeds(sourcepath: String) = {
    val list = scala.io.Source.fromFile(sourcepath).getLines.map(line => {
      val tuples = line.split("\t")
      (tuples(0), tuples(1))
    }).toList
    val disrel_lexicallist_map = list.groupBy(_._1).mapValues(_.map(_._2))
    disrel_lexicallist_map
  }

  /** Input: set of specific dependency-paths
    * Output: set of general dependency-paths (replace specific-node with "_1, _2, _3, ...")
    */
  def getGeneralDependencySets(dependencies: List[(Int, Int, Set[Polyparser.Myedge])]) = {
    val generalDependencies = Polyparser.generalizeDependencypaths(dependencies)
    generalDependencies
  }

  /** Get dependency-path with specific length defined in the argument
    * Input: root, tree, arg1list, arg2list, pathlength
    * Output: List[Set[edges]]
    */
  def getpathwithspecificlength(root: Polyparser.Mytokennode, tree: Polyparser.Mygraph,
    arg1list: List[Int], arg2list: List[Int], pathlength: Int) = {
    //org.allenai.nlpstack.graph.Graph.Edge[org.allenai.nlpstack.parse.graph.TokenDependencyNode]
    var pathlist: List[(Int, Int, Set[Polyparser.Myedge])] = List()
    arg1list.foreach(arg1 => arg2list.foreach(arg2 => {
      val (flag, pathsets) = Polyparser.findDepPathWithSpeLen(tree.vertices.toList, tree.edges, arg1, arg2, pathlength)
      if (flag == true) {
        pathsets.foreach(p => pathlist = pathlist ::: List((arg1, arg2, p)))
      }
    }))
    pathlist
  }

  /** If there exists an edge named as "prep", find the pos of its second argument
    */
  def getlemmaofprep(lengthDependenciesMap: Map[Integer, List[(Int, Int, Set[Polyparser.Myedge])]]) = {
    var lemmaofprep: Set[String] = Set()
    lengthDependenciesMap.foreach {
      case (length, dependencies) => {
        dependencies.foreach {
          case (arg1, arg2, edges) => {
            edges.foreach {
              case edge =>
                if (edge.label.equals("prep")) {
                  lemmaofprep = lemmaofprep ++ Set(edge.dest.lemma)
                }
            }
          }
        }
      }
    }
    lemmaofprep
  }

  /** Given all dependencies paths, find the connection-words that connect arguments
    */
  def getconnectwords(lengthDependenciesPath: Map[Integer, List[(Int, Int, Set[Polyparser.Myedge])]]) = {
    var connectwords: Set[String] = Set()
    lengthDependenciesPath.foreach {
      case (length, dependency) => {
        dependency.foreach {
          case (arg1, arg2, d) => {
            d.foreach(edge => {
              if (edge.source.id != arg1 && edge.source.id != arg2) connectwords = connectwords + edge.source.string;
              if (edge.dest.id != arg1 && edge.dest.id != arg2) connectwords = connectwords + edge.dest.string
            })
          }
        }
      }
    }
    connectwords
  }
}
